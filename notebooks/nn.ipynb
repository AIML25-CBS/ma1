{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "## Part I: Multi-layer Perceptron\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please fill in the following information:*\n",
    "\n",
    "Name: <Your name> <br>\n",
    "Student ID: <Your student ID> <br>\n",
    "Group: <Your group number>\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Traditional machine learning methods, like logistic regression in Scikit-Learn, are often sufficient for datasets that are linearly separable. However, more complex problems sometimes demand a more intricate approach. Neural networks, which incorporate additional layers, excel at learning these complex relationships, leading to improved performance. These extra layers, known as \"hidden\" layers, process the input into one or more intermediate forms before generating the final prediction.\n",
    "\n",
    "Logistic regression achieves this transformation using a single fully-connected layer, also referred to as a Single-Layer Perceptron. This layer performs a linear transformation (a matrix multiplication combined with a bias). In contrast, a neural network with multiple connected layers is typically referred to as a Multi-Layer Perceptron (MLP). For instance, in the simple MLP shown below, a 4-dimensional input is mapped to a 5-dimensional hidden representation, which is subsequently transformed into a single output used for prediction.\n",
    "\n",
    "<img src=\"../media/MLP.png\" width=\"500\"/>\n",
    "\n",
    "In this assignment, your task will be to construct an MLP for the well-known MNIST dataset.\n",
    "\n",
    "#### Nonlinearities revisited\n",
    "\n",
    "WNonlinearities are usually applied between the layers of a neural network. As discussed in class 2, there are several reasons for this. A key reason is that without any nonlinearity, a sequence of linear transformations (fully connected layers) reduces to a single linear transformation, limiting the model's expressiveness to that of a single layer. Including nonlinearities between layers prevents this reduction, enabling neural networks to approximate far more complex functions. This is what makes neural networks so powerful.\n",
    "\n",
    "Numerous nonlinear activation functions are frequently employed in neural networks, but one of the most commonly used is the [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)):\n",
    "\n",
    "\\begin{align}\n",
    "x = \\max(0,x)\n",
    "\\end{align}\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Build a 2-layer MLP for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "* Image (784 dimensions) ->  \n",
    "* fully connected layer (500 hidden units) -> \n",
    "* nonlinearity (ReLU) ->  \n",
    "* fully connected (10 hidden units) -> \n",
    "* softmax\n",
    "\n",
    "\n",
    "*Some hints*:\n",
    "- Even as we add additional layers, we still only require a single optimizer to learn the parameters. \n",
    "- To get the best performance, you may want to play with the learning rate and the number of training epochs.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary imports\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# sklearn for metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate GPU\n",
    "If available. Note that this is not necessary, but it will speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "if torch.backends.mps.is_available():  # GPU on MacOS\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():  # GPU on Linux/Windows\n",
    "    device = \"cuda\"\n",
    "else:  # default to CPU if no GPU is available\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "print(f\"Running pytorch version {torch.__version__}) with backend = {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\n",
    "    root = 'data',  # The root directory where the dataset will be stored\n",
    "    download = True,  # If the dataset is not found at root, it will be downloaded\n",
    "    train = True,  # The train dataset (as opposed to the test dataset)\n",
    "    transform = transforms.ToTensor()  # transformations to be applied to the dataset, in this case, convert the images to tensors\n",
    ")\n",
    "test = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train,  # The dataset\n",
    "    batch_size = 10,  # The size of each batch (10 images in this case)\n",
    "    shuffle = False # Whether to shuffle the dataset\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test,\n",
    "    batch_size = 10,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random example from the training set\n",
    "selection = random.randrange(len(train)-1)\n",
    "image, label = train[selection]\n",
    "\n",
    "# Plot the image\n",
    "print(f\"Default image shape: {image.shape}\")\n",
    "image = image.view([28,28])\n",
    "\n",
    "print(f\"Reshaped image shape: {image.shape}\")\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "# Print the label\n",
    "print(f\"The label for this image: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # TODO: Define the layers of the network\n",
    "  \n",
    "  def forward(self, X : torch.Tensor):\n",
    "\n",
    "    # TODO: Define the forward pass of the network\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = ...  # TODO: set the learning rate\n",
    "EPOCHS = ... # TODO: set the number of epochs (i.e. passes over the dataset)\n",
    "LOSS = nn.CrossEntropyLoss()  # the loss function - We suggest using CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()  # Create an instance of the MLP model\n",
    "mlp.to(device)  # Move the model to the device (GPU or CPU)\n",
    "\n",
    "optimizer = torch.optim.SGD(  # The optimizer\n",
    "   mlp.parameters(),\n",
    "   lr=LR,\n",
    "   # Feel free to experiment with other parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp.train() # Set the model to training mode\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0  # i.e. the loss for this epoch\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        \n",
    "        images, labels = batch  # unpack images and labels from the batch\n",
    "\n",
    "        images = images.to(device) # Move the images to GPU if available\n",
    "        labels = labels.to(device) # Move the labels to GPU if available\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()  # Zero the gradients, i.e. reset the gradients to zero so that they don't accumulate between batches\n",
    "\n",
    "        # Forward pass\n",
    "        images = images.view(-1, 28 * 28)  # Flatten the images\n",
    "        outputs = mlp()  # Forward pass the images through the model\n",
    "\n",
    "        # Compute loss\n",
    "        loss = LOSS(outputs, labels)  # Compute the loss\n",
    "\n",
    "        running_loss += loss.item()  # Add the loss to the running loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()  # Compute the gradients\n",
    "        optimizer.step() # Update the weights of the model using the gradients\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop\n",
    "mlp.eval()  # Set model to evaluation mode\n",
    "y_true = []  # To store true labels\n",
    "y_pred = []  # To store predicted labels\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking, as we are not updating the model parameters\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        images = images.view(-1, 28 * 28) # Flatten the images to 1D\n",
    "        outputs = mlp(images)  # Forward pass\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Store labels for the classification report\n",
    "        y_true.extend(labels.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        y_pred.extend(predictions.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(10)]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
